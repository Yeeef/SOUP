# lexer notations

[extremely nice tutorial](http://www.dabeaz.com/ply/ply.html)

## token list definition

### pattern matching and actions

### t_ignore

å€¼å¾—æ³¨æ„çš„æ˜¯ `t_ignore` ä¸éœ€è¦ `r` æ¥ä¿®é¥°ï¼Œit expects a string of *literal chars* to be ignored, not a regex.

å¦‚æœè¦å¿½ç•¥å›è½¦ï¼Œåˆ¶è¡¨ç¬¦ï¼Œç©ºæ ¼ï¼Œåˆ™ `t_ignore = ' \t\n'` å³å¯

ğŸ‘‰ [related issue](https://github.com/dabeaz/ply/issues/126)

### discarded tokens

è¿™ä¸ªå¯èƒ½ä¸ `t_ignore` æœ‰æ‰€æ··æ·†ï¼Œæˆ‘ä¸ªäººæ„Ÿè§‰åŠŸèƒ½æœ‰æ‰€é‡å ï¼Œå» [extremely nice tutorial](http://www.dabeaz.com/ply/ply.html) çœ‹ï¼Œä¸èµ˜è¿°ã€‚æ€»ä¹‹è¿™ä¸ªç”¨äºå®ç°æ›´åŠ å¤æ‚çš„ ignore, ä¹‹å‰çš„æ•ˆç‡å¾ˆé«˜

### attention âš ï¸

- All tokens defined by functions are added in the same order as they appear in the lexer file.
- Tokens defined by strings are added next by sorting them in order of decreasing regular expression length (longer expressions are added first).

### reserved word attentions âš ï¸

```python
reserved = {
   'if' : 'IF',
   'then' : 'THEN',
   'else' : 'ELSE',
   'while' : 'WHILE',
   ...
}

tokens = ['LPAREN','RPAREN',...,'ID'] + list(reserved.values())

def t_ID(t):
    r'[a-zA-Z_][a-zA-Z_0-9]*'
    t.type = reserved.get(t.value,'ID')    # Check for reserved words
    return t
```

ä¸Šé¢çš„ç¨‹åºç¡®ä¿ *å…³é”®å­—* çš„æ­£ç¡®å®šä¹‰ä¸åŒ¹é…ï¼Œ**æ³¨æ„å…³é”®å­—ä¸è¦ç”¨ `t_IF = r'if'` è¿™ç§è¯­æ³•å†™**

### @TOKEN decorator ğŸ‘

```python
from ply.lex import TOKEN

digit            = r'([0-9])'
nondigit         = r'([_A-Za-z])'
identifier       = r'(' + nondigit + r'(' + digit + r'|' + nondigit + r')*)'

@TOKEN(identifier)
def t_ID(t):
    ...
```

## usage

- t.value å­˜å‚¨ç€ match çš„å­—ç¬¦ä¸²



